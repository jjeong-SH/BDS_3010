{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "perfect-confusion",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 기사 크롤링\n",
    "- 1) \"코로나\" 키워드로 검색 후 검색결과를 크롤링해서 기사링크 리스트를 만든다.\n",
    "    - max page = 100을 설정해서, 하루당 1000건의 뉴스 (=100페이지*한 페이지당 10건의 뉴스)를 크롤링한다\n",
    "- 2) 크롤링한 기사들의 네이버 뉴스 링크로 다시 크롤링해서 들어간다\n",
    "    - 네이버 뉴스에서 제공하는 기사들이므로 같은 html 구조를 가지고 있어서, 같은 크롤링함수를 적용가능하다\n",
    "    - 각 뉴스 기사에서 타이틀, 발행일, 기사 내용을 크롤링한다\n",
    "    \n",
    "참고: https://bumcrush.tistory.com/155"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-certification",
   "metadata": {},
   "source": [
    "### 네이버의 API로 데이터를 받아올 때의 주의사항\n",
    "\n",
    "- 검색 API를 사용\n",
    "- 검색 결과로부터 최대 1000개까지의 결과만 가져옴\n",
    "- 뉴스기사의 본문을 전부 가져오지는 못함. 대략 초입 3줄 정도?\n",
    "- 일 최대 25,000번의 호출 가능\n",
    "- 한 번 API 호출시 최대 100개의 결과값 수집 가능\n",
    "- 최대로 수집해봤자 일부 중복되는 결과 발생\n",
    "\n",
    "\n",
    "출처: https://signing.tistory.com/76 [끄적거림]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "black-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.makedirs('./crawler', exist_ok=True)\n",
    "os.chdir('./crawler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jeongjaewon/Desktop/1조/crawler'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dried-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_crawler(maxpage, query, date):\n",
    "    '''\n",
    "    1) 코로나 키워드로 검색한 결과에서 기사들의 네이버 뉴스 링크를 크롤링하는 함수\n",
    "    '''\n",
    "    import os\n",
    "    os.makedirs('./files', exist_ok=True)\n",
    "    \n",
    "    date_ = date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1 \n",
    "    url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + date + \"&de=\" + date + \"&nso=so%3Ar%2Cp%3Afrom\" + date_ + \"to\" + date_ + \"%2Ca%3A&start=\" + str(page)\n",
    "    url_link_list=[] #코로나 검색결과를 저장하는 리스트\n",
    "\n",
    "    while page < maxpage_t:\n",
    "        if page//100>0 and page%100==1:\n",
    "            print(f'{page}page', end=' - ')\n",
    "        # get requests\n",
    "        try:\n",
    "            request=requests.get(url)\n",
    "        except:\n",
    "            print(f'Connection Error: {requests.status_code}')\n",
    "        #get content and parse\n",
    "        content = request.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        #get link from each of the news sub box\n",
    "        for sub_bx in soup.find_all(\"li\", class_=\"sub_bx\"):\n",
    "            try:\n",
    "                link = sub_bx.find(\"a\")['href']\n",
    "                url_link_list.append(link)\n",
    "            except:\n",
    "                pass\n",
    "        page += 10\n",
    "    return url_link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "possible-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(url_link_list):   \n",
    "    '''\n",
    "    link_crawler 함수의 결과를 저장하는 함수\n",
    "    '''\n",
    "    with open('./files/url_link.txt', 'w') as f:\n",
    "        for link in url_link_list:\n",
    "            f.write(link+'\\n')\n",
    "    print('url writing Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-shipping",
   "metadata": {},
   "source": [
    "### 이슈발생1\n",
    "requests 라이브러리를 그냥 쓴다고 손쉽게 HTTP 호출을 할 수 있는 것은 아니다. 무분별한 크롤링을 막기 위해 이런 저런 장치가 되어 있어서, 가급적 브라우저를 통한 접속과 구분할 수 없게 일종의 위장이 필요하다.\n",
    "\n",
    "\n",
    "\n",
    "출처: https://butnotforme.tistory.com/entry/python으로-업무-자동화까지-8-requests3?category=932590 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mexican-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_crawler(url_link_list):\n",
    "    '''\n",
    "    2) 크롤링한 기사들의 네이버 뉴스 링크로 하나씩 들어가서 뉴스 기사(제목, 발행일, 내용)를 크롤링하는 함수\n",
    "    '''\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    news_title=[]\n",
    "    published_date=[]\n",
    "    news_text=[]\n",
    "    #이슈 발생1 참고\n",
    "    headers = { \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\" }\n",
    "\n",
    "    for link in tqdm(url_link_list):\n",
    "        try:\n",
    "            r = requests.get(link, headers=headers)\n",
    "        except:\n",
    "            print('Connection Error')\n",
    "        if r:\n",
    "            try:\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "                title = soup.select('h3#articleTitle')[0].text\n",
    "                news_title.append(title)\n",
    "                pdate = soup.select('.t11')[0].get_text()[:11]\n",
    "                published_date.append(pdate)\n",
    "                text = soup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \").strip()\n",
    "                text = text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "                news_text.append(text.strip())\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "    assert len(news_title)==len(published_date)==len(news_text)\n",
    "    result = pd.DataFrame({'title': news_title, 'publish':published_date, 'news_text':news_text})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-authority",
   "metadata": {},
   "source": [
    "### 이슈 발생2\n",
    "- 기간 별로 뉴스를 긁어올때, maxpage안에서만 긁어오게 되어있음\n",
    "- 이떄 정렬 기준이 최신순이기 때문에 03.10~03.03 순으로 정렬되어 긁어오게 됨\n",
    "- 그러나 해당 기간의 뉴스가 너무 많이 나와 max page=100으로는 03.10에 발행된 뉴스를 긁어오는 것 밖에 안됨\n",
    "- 또한, 임의로 max page를 정해서 긁어오게 된다면, 날짜 별로 다른 갯수의 기사를 긁어오게됨\n",
    "- 예를 들어, 03.10일에는 500개(전체 발행기사), 03.09일에는 400개(전체 발행기사), 03.08일에는 100개\n",
    "- 이는 표본 바이어스 문제가 발생하게 된다.\n",
    "- **따라서, 각 날짜별로 각각 max page=100으로 설정해서 긁어오도록 한다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "herbal-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_time(func):\n",
    "    import time\n",
    "    def wrapper(*args, **kwargs):\n",
    "        st=time.time()\n",
    "        res = func(*args, **kwargs)\n",
    "        print(round(time.time()-st, 2),'초 소요')\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amended-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@check_time\n",
    "def main_crawler(maxpage, query, s_date, e_date, save=True):\n",
    "    '''\n",
    "    link_crawler와 news_crawler를 동작하게 하는 메인 크롤러 함수\n",
    "    '''\n",
    "    from datetime import datetime, timedelta    \n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    url_link_list = []\n",
    "    \n",
    "    start=datetime.strptime(s_date, '%Y-%m-%d')\n",
    "    end=datetime.strptime(e_date, '%Y-%m-%d')\n",
    "    period=(end-start).days\n",
    "    \n",
    "    #이슈발생2 참고\n",
    "    for i in range(period+1):\n",
    "        date = start + timedelta(i)\n",
    "        date = date.strftime('%Y.%m.%d')\n",
    "        print(date, 'news link crawling')\n",
    "        url_link_list.extend(link_crawler(maxpage,query,date))\n",
    "        if save:\n",
    "            write_file(url_link_list)\n",
    "      \n",
    "    result=news_crawler(url_link_list)\n",
    "    data = pd.concat([data,result], axis=0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-referral",
   "metadata": {},
   "source": [
    "첫번째 주기 [2020-03-03 ~ 2020-03-10]\n",
    "두번째 주기 [2020-08-15 ~ 2020-09-02]\n",
    "세번째 주기 [2020-12-15 ~ 2021-01-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "injured-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(maxpage, s_date,e_date):\n",
    "    maxpage, query, s_date, e_date = maxpage, '코로나', s_date, e_date\n",
    "    data = main_crawler(maxpage, query, s_date, e_date)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "freelance-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.03.03 news link crawling\n",
      "url writing Done\n",
      "2020.03.04 news link crawling\n",
      "url writing Done\n",
      "2020.03.05 news link crawling\n",
      "url writing Done\n",
      "2020.03.06 news link crawling\n",
      "url writing Done\n",
      "2020.03.07 news link crawling\n",
      "url writing Done\n",
      "2020.03.08 news link crawling\n",
      "url writing Done\n",
      "2020.03.09 news link crawling\n",
      "url writing Done\n",
      "2020.03.10 news link crawling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/119 [00:00<00:22,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url writing Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:22<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.53 초 소요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publish</th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>코로나19 확진 374명 추가·총 5천186명…43일만에 5천명 넘어</td>\n",
       "      <td>2020.03.03.</td>\n",
       "      <td>3일 0시 기준 대구·경북 총 4천286명·신천지 관련 총 2천698명오늘도 분주한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>코로나19로 일자리 없는 제주 中불법체류자들 \"차라리 귀국\"(종합)</td>\n",
       "      <td>2020.03.03.</td>\n",
       "      <td>법무부 자진출국 유도 지난달 230명 이상 출국신고, 3일 250여명 몰려…역대 최...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>코로나19 무서워…꼭꼭 숨었던 불법체류자 '탈'제주</td>\n",
       "      <td>2020.03.03.</td>\n",
       "      <td>코로나19 국내 확산세로 불법체류 중국인 자진출국 잇따라[제주CBS 고상현 기자]코...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"나 돌아갈래\" 코로나19로 제주 中불법체류자 수백명 탈출 행렬</td>\n",
       "      <td>2020.03.03.</td>\n",
       "      <td>법무부 자진출국 유도 지난달 230명 이상 출국신고, 3일 하루 200여명 몰려 코...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“일자리 없어 다시 가요” 코로나 여파 제주 떠나는 불법체류자들</td>\n",
       "      <td>2020.03.03.</td>\n",
       "      <td>제주출입국·외국인청에 중국인 불법체류자 출국 신청 봇물3일 제주출입국외국인청 건물 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title      publish  \\\n",
       "0  코로나19 확진 374명 추가·총 5천186명…43일만에 5천명 넘어  2020.03.03.   \n",
       "1   코로나19로 일자리 없는 제주 中불법체류자들 \"차라리 귀국\"(종합)  2020.03.03.   \n",
       "2            코로나19 무서워…꼭꼭 숨었던 불법체류자 '탈'제주  2020.03.03.   \n",
       "3     \"나 돌아갈래\" 코로나19로 제주 中불법체류자 수백명 탈출 행렬  2020.03.03.   \n",
       "4     “일자리 없어 다시 가요” 코로나 여파 제주 떠나는 불법체류자들  2020.03.03.   \n",
       "\n",
       "                                           news_text  \n",
       "0  3일 0시 기준 대구·경북 총 4천286명·신천지 관련 총 2천698명오늘도 분주한...  \n",
       "1  법무부 자진출국 유도 지난달 230명 이상 출국신고, 3일 250여명 몰려…역대 최...  \n",
       "2  코로나19 국내 확산세로 불법체류 중국인 자진출국 잇따라[제주CBS 고상현 기자]코...  \n",
       "3  법무부 자진출국 유도 지난달 230명 이상 출국신고, 3일 하루 200여명 몰려 코...  \n",
       "4  제주출입국·외국인청에 중국인 불법체류자 출국 신청 봇물3일 제주출입국외국인청 건물 ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maxpage =2로 설정해서 확인해봄\n",
    "data = main(2, '2020-03-03', '2020-03-10')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disciplinary-berry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.03.03 news link crawling\n",
      "url writing Done\n",
      "2020.03.04 news link crawling\n",
      "url writing Done\n",
      "2020.03.05 news link crawling\n",
      "url writing Done\n",
      "2020.03.06 news link crawling\n",
      "url writing Done\n",
      "2020.03.07 news link crawling\n",
      "url writing Done\n",
      "2020.03.08 news link crawling\n",
      "url writing Done\n",
      "2020.03.09 news link crawling\n",
      "url writing Done\n",
      "2020.03.10 news link crawling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url writing Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:28<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.55 초 소요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(2, '2020-03-03', '2020-03-10').to_csv('./files/temp.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-express",
   "metadata": {},
   "source": [
    "### 참고\n",
    "- 각 날짜별로 크로링된 기사수는 다를 수 있음(모든 뉴스기사를 네이버 뉴스에서 제공하지 않기 때문임)\n",
    "- 예를 들어, 네이버 연예에서 제공하는 뉴스는 네이버 뉴스와 html 구조가 다르므로, 이럴 경우 pass함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-exploration",
   "metadata": {},
   "source": [
    "# 주의!! 아래의 코드를 실행하면 크롤링이 시작됩니다.\n",
    "# 굉장히 오랜 시간(약 3-4시간) 이 걸리므로, 주의하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(100, '2020-03-03', '2020-03-10').to_csv('./files/1st.csv')\n",
    "main(100, '2020-08-15', '2020-09-02').to_csv('./files/2nd.csv', index=None)\n",
    "main(100, '2020-12-15', '2020-01-10').to_csv('./files/3rd.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-shade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
